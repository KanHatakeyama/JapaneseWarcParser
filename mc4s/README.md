# mc4データセットをクリーニングするスクリプト
- Huggingfaceのdatasetsから読み込み
    - 各レコードごとに
        - 正規化
        - 文章クリーニング
        - 機械学習による選別(教師有り)
        - (記事内容のクラス分け(教師なし) : 精度が悪いので無し)
        - 出力

# 実行方法
```
python mc4_cleaner.py
```
- 備考
    - はじめに､[clean_dev.ipynb](clean_dev.ipynb)でfasttextのモデルを作ります.
    - 100 it/sほどの速度で動きます｡
        - 87337884 recordあるので､240 hrほどかかりそうです｡
    - 並列化､高速化の余地は沢山あると思います
        - mc4 datasetを一時ファイルとして､iteratorで読み込んでるので､ここが明らかな律速段階の一つです｡


# TODO
- 並列化
- datasetsライブラリへのラッピング
    - datasetsライブラリを継承したクラスを作る
        - cleaned_mc4_dataset 的なやつ
    - __init__を呼び出した時に､フォルダを生成し､一連の前処理を施す
    - それ以降は､ dataset[0] 的な感じで中身を呼び出せるようにしたい
- mc4以外のデータセット(例えばoscar)でも同じように処理を施す